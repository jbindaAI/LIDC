{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7dc4987-be8e-4a0b-b1d6-72ca9be8d3b6",
   "metadata": {},
   "source": [
    "# Finetuning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75cfa6-8dce-4c70-92eb-eddab1156c2a",
   "metadata": {},
   "source": [
    "Fine-tuning ViT models with PyTorch Lightning framework. <br/>\n",
    "End2End model is intended to predict whether nodule on a given scan crop is malignant or benign. Prediction is done soley on the basis of images.<br/>\n",
    "Second model, Concept model given a scan crop predict typical attributes of nodule (concepts) like: Diameter, Calcification, Subtlety and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c29f96-5157-4dc6-9a52-b0b43ef9b9e5",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b8cd33-9c08-4890-b891-6154217c3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66954b9f-9266-4da5-a44e-ef2f614f03fd",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4e85fc-90f9-4a3f-bf8b-ec68f2648dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from LIDC_Data_Module import LIDC_Data_Module\n",
    "from ViT_End2End_Model_base import ViT_End2End_Model_base\n",
    "from ViT_Concept_Model_base import ViT_Concept_Model_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a900bba-af0a-4990-be81-e20871f837dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset\"\n",
    "weights_path = \"weights\"\n",
    "tb_logs_path = \"tb_logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053dcb2c-1da3-43ab-8992-de3c67e44367",
   "metadata": {},
   "source": [
    "## Finetuning benign/malignant End2End ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49085ea-06f1-43e4-9774-9a8e40d5f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT_end2end_32p\"\n",
    "Path(f\"{weights_path}/End2End_ViT/{model_name}\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{tb_logs_path}/End2End_ViT/{model_name}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029cbe9-5a70-4533-b600-73fa250007a7",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training Loop\n",
    "for fold in range(5): # 5 cross validation folds\n",
    "    print(f\"fold: {fold}\")\n",
    "    trainer = Trainer(\n",
    "        devices=1,\n",
    "        accelerator='gpu',\n",
    "        max_epochs=50,\n",
    "        log_every_n_steps=22,\n",
    "        logger=TensorBoardLogger(f\"{tb_logs_path}/End2End_ViT/{model_name}\", name=f\"fold_{fold+1}\"),\n",
    "        enable_checkpointing=False\n",
    "    )\n",
    "    \n",
    "    model = ViT_End2End_Model_base(learning_rate=1e-3, freeze_layers=True, train_layers=40, patch_size=32) \n",
    "    data_module = LIDC_Data_Module(\n",
    "        data_dir=data_path,\n",
    "        fold=fold,\n",
    "        batch_size=32,\n",
    "        num_workers=8,\n",
    "        apply_mask=False,\n",
    "        finetuning=True\n",
    "    )\n",
    "    trainer.fit(model, data_module)\n",
    "    torch.save(model.state_dict(), f\"{weights_path}/End2End_ViT/{model_name}/ViT_finetune_{fold}.pt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db8ef4-0dba-4b61-a82d-0fcff6482196",
   "metadata": {},
   "source": [
    "## Finetuning ViT Concept Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a5b5ab3-5ae0-4a3e-898a-17155c6a7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT_concept_32p\"\n",
    "Path(f\"{weights_path}/Concept_ViT/{model_name}\").mkdir(parents=True, exist_ok=True)\n",
    "Path(f\"{tb_logs_path}/Concept_ViT/{model_name}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9d624-5c64-4533-a133-fbf4c23b4da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training Loop\n",
    "for fold in range(5): # 5 cross validation folds\n",
    "    print(f\"fold: {fold}\")\n",
    "    trainer = Trainer(\n",
    "        devices=1,\n",
    "        accelerator='gpu',\n",
    "        max_epochs=50,\n",
    "        log_every_n_steps=22,\n",
    "        logger=TensorBoardLogger(f\"{tb_logs_path}/Concept_ViT/{model_name}\", name=f\"fold_{fold+1}\"),\n",
    "        enable_checkpointing = False\n",
    "    )\n",
    "    \n",
    "    model_concepts_ViT = ViT_Concept_Model_base(learning_rate=1e-3, train_layers=40, freeze_layers=True, patch_size=32)\n",
    "    data_module = LIDC_Data_Module(\n",
    "        data_dir=data_path,\n",
    "        fold=fold,\n",
    "        labels=\"concepts\",\n",
    "        batch_size=32,\n",
    "        num_workers=8,\n",
    "        apply_mask=False,\n",
    "        finetuning=True\n",
    "    )\n",
    "    trainer.fit(model_concepts_ViT, data_module)\n",
    "    torch.save(model_concepts_ViT.state_dict(), f\"{weights_path}/Concept_ViT/{model_name}/concept_ViT_finetune_{fold}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63721509-6875-4c9b-98aa-1a605264dccb",
   "metadata": {},
   "source": [
    "## Evaluating End2End ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a21e551d-ac91-4b9b-a18a-4a592490910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT_end2end_32p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfebee-c1ff-4ef2-a51d-f29fc07e4525",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(devices=1,\n",
    "                 accelerator=\"gpu\"\n",
    "                 )\n",
    "acc = []\n",
    "auroc = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for fold in range(5):\n",
    "    print(f\"fold: {fold}\")\n",
    "    data_module = LIDC_Data_Module(data_dir=data_path,\n",
    "                             fold=fold,\n",
    "                             batch_size=32,\n",
    "                             num_workers=8,\n",
    "                             apply_mask=False,\n",
    "                             finetuning=True\n",
    "                            )\n",
    "    \n",
    "    model =  ViT_End2End_Model_base()\n",
    "    model.load_state_dict(torch.load(f\"weights/End2End_ViT/{model_name}/ViT_finetune_{fold}.pt\"))\n",
    "    model.eval()\n",
    "    res = trainer.test(model, data_module)\n",
    "    acc.append(res[0][\"val_acc\"])\n",
    "    auroc.append(res[0][\"val_auroc\"])\n",
    "    precision.append(res[0][\"val_precision\"])\n",
    "    recall.append(res[0][\"val_recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1d0f3-aeed-4918-b52e-94f44c9d412b",
   "metadata": {},
   "source": [
    "### Average Results of the End2End ViT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf81384-c15e-4685-af84-bbe666dd8730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8720391392707825"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a23c4a2-e6d8-40b0-9142-76d6736a718f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8875610709190369"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "572df1a7-3792-41fd-b1ce-58fa96e71a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8986511468887329"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c9003a-851b-475a-ba6a-e07359150fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
